<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Foundation on Awyugan&#39;s Blog</title>
    <link>https://awyugan.github.io/tags/foundation/</link>
    <description>Recent content in Foundation on Awyugan&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 06 Oct 2023 18:03:40 +0800</lastBuildDate>
    <atom:link href="https://awyugan.github.io/tags/foundation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>【译】Large Transformer Model 推理优化 | Lil&#39;Log</title>
      <link>https://awyugan.github.io/2023/10/06/%E8%AF%91large-transformer-model-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96-lillog/</link>
      <pubDate>Fri, 06 Oct 2023 18:03:40 +0800</pubDate>
      <guid>https://awyugan.github.io/2023/10/06/%E8%AF%91large-transformer-model-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96-lillog/</guid>
      <description>[January 10, 2023] · 31 min · Lilian Weng 2023-01-24 更新：增加了关于蒸馏 的小节 如今，大型 Transformer 模型已经成为主流，为各种任务创造了 SoTA (最先进的技术) 结果。它们功能强大但训练和使用</description>
    </item>
    <item>
      <title>【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log</title>
      <link>https://awyugan.github.io/2023/10/06/%E8%AF%91%E5%A6%82%E4%BD%95%E5%9C%A8%E5%A4%9A-gpu-%E4%B8%8A%E8%AE%AD%E7%BB%83%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%A4%A7%E5%9E%8B%E6%A8%A1%E5%9E%8B-lillog/</link>
      <pubDate>Fri, 06 Oct 2023 03:03:40 +0800</pubDate>
      <guid>https://awyugan.github.io/2023/10/06/%E8%AF%91%E5%A6%82%E4%BD%95%E5%9C%A8%E5%A4%9A-gpu-%E4%B8%8A%E8%AE%AD%E7%BB%83%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%A4%A7%E5%9E%8B%E6%A8%A1%E5%9E%8B-lillog/</guid>
      <description>2022-03-13 更新: 添加 expert choice routing 2022-06-10 更新: Greg 和我撰写了这篇文章的缩短和升级版本，发表在 OpenAI Blog 上：&amp;ldquo;训练大型神经网络的技术&amp;rdquo; 近年来，我们</description>
    </item>
  </channel>
</rss>
