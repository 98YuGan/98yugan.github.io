<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log | Awyugan&#39;s Blog</title>
<meta name="keywords" content="architecture, transformer, foundation">
<meta name="description" content="2022-03-13 更新: 添加 expert choice routing 2022-06-10 更新: Greg 和我撰写了这篇文章的缩短和升级版本，发表在 OpenAI Blog 上：&ldquo;训练大型神经网络的技术&rdquo; 近年来，我们">
<meta name="author" content="
作者:&nbsp;Lilian Weng">
<link rel="canonical" href="https://awyugan.github.io/2023/10/06/%E8%AF%91%E5%A6%82%E4%BD%95%E5%9C%A8%E5%A4%9A-gpu-%E4%B8%8A%E8%AE%AD%E7%BB%83%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%A4%A7%E5%9E%8B%E6%A8%A1%E5%9E%8B-lillog/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f609105c87baf33321189c18cb2cb85612ccb41c03a16449e1f731b7aad3e1c4.css" integrity="sha256-9gkQXIe68zMhGJwYyyy4VhLMtBwDoWRJ4fcxt6rT4cQ=" rel="preload stylesheet" as="style">


<head>
  
  <script type="text/javascript">
  MathJax = {
    tex: {
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      inlineMath: [['$', '$'], ['\\(', '\\)']],
    },
  };
</script>
<script
    async
    id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"
    integrity="sha384-+BSz3oj3ILMYvOBr16U9i0H4RZRmGyQQ+1q9eqr8T3skmAFrJk8GmgwgqlCZdNSo"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>
</head>
<link rel="icon" href="https://awyugan.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="16x16" href="https://awyugan.github.io/img/Q.gif">
<link rel="icon" type="image/png" sizes="32x32" href="https://awyugan.github.io/img/Q.gif">
<link rel="apple-touch-icon" href="https://awyugan.github.io/Q.gif">
<link rel="mask-icon" href="https://awyugan.github.io/Q.gif">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
	
	<style>
	.img-shadow {
	    box-shadow: 8px 8px 10px rgba(0, 0, 0, 0.5);  
	}
	</style>
	
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<script type="text/javascript">
  MathJax = {
    tex: {
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      inlineMath: [['$', '$'], ['\\(', '\\)']],
    },
  };
</script>
<script
    async
    id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"
    integrity="sha384-+BSz3oj3ILMYvOBr16U9i0H4RZRmGyQQ+1q9eqr8T3skmAFrJk8GmgwgqlCZdNSo"
    crossorigin="anonymous"
    referrerpolicy="no-referrer"
    type="text/javascript"></script>

>
<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
<meta property="og:title" content="【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log" />
<meta property="og:description" content="2022-03-13 更新: 添加 expert choice routing 2022-06-10 更新: Greg 和我撰写了这篇文章的缩短和升级版本，发表在 OpenAI Blog 上：&ldquo;训练大型神经网络的技术&rdquo; 近年来，我们" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://awyugan.github.io/2023/10/06/%E8%AF%91%E5%A6%82%E4%BD%95%E5%9C%A8%E5%A4%9A-gpu-%E4%B8%8A%E8%AE%AD%E7%BB%83%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%A4%A7%E5%9E%8B%E6%A8%A1%E5%9E%8B-lillog/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-10-06T03:03:40+08:00" />
<meta property="article:modified_time" content="2023-10-06T03:03:40+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log"/>
<meta name="twitter:description" content="2022-03-13 更新: 添加 expert choice routing 2022-06-10 更新: Greg 和我撰写了这篇文章的缩短和升级版本，发表在 OpenAI Blog 上：&ldquo;训练大型神经网络的技术&rdquo; 近年来，我们"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://awyugan.github.io/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "【译】如何在多 GPU 上训练真正的大型模型？ | Lil'Log",
      "item": "https://awyugan.github.io/2023/10/06/%E8%AF%91%E5%A6%82%E4%BD%95%E5%9C%A8%E5%A4%9A-gpu-%E4%B8%8A%E8%AE%AD%E7%BB%83%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%A4%A7%E5%9E%8B%E6%A8%A1%E5%9E%8B-lillog/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "【译】如何在多 GPU 上训练真正的大型模型？ | Lil'Log",
  "name": "【译】如何在多 GPU 上训练真正的大型模型？ | Lil\u0027Log",
  "description": "2022-03-13 更新: 添加 expert choice routing 2022-06-10 更新: Greg 和我撰写了这篇文章的缩短和升级版本，发表在 OpenAI Blog 上：\u0026ldquo;训练大型神经网络的技术\u0026rdquo; 近年来，我们",
  "keywords": [
    "architecture", "transformer", "foundation"
  ],
  "articleBody": "2022-03-13 更新: 添加 expert choice routing 2022-06-10 更新: Greg 和我撰写了这篇文章的缩短和升级版本，发表在 OpenAI Blog 上：“训练大型神经网络的技术” 近年来，我们在许多 NLP 基准任务上看到，更大的预训练语言模型 能够取得更好的结果。但是，训练大型和深层神经网络具有挑战性，因为它需要大量的 GPU 内存和长时间的训练周期。\n然而，单个 GPU 工作器的内存是有限的，许多大型模型的大小已经超过了单个 GPU 的容量。有几种并行计算范式可以实现跨多个 GPU 的模型训练，还有各种模型架构和内存节省设计，以帮助训练非常大的神经网络成为可能。\n训练并行性 训练非常大的神经网络模型的主要瓶颈是对大量 GPU 内存的强烈需求，远远超过单个 GPU 机器所能承载的。除了模型权重（例如，数百亿的浮点数）之外，通常存储中间计算输出（如梯度和优化器状态（例如，Adam 中的动量和变异））的成本更高。此外，训练大型模型通常需要配备大型训练语料库，因此单个过程可能需要很长时间。\n因此，必须采用并行计算。并行计算可以在不同维度进行，包括数据、模型架构和张量运算。\n数据并行 最简单的 数据并行（DP） 方法是将相同的模型权重复制到多个工作节点，并将一部分数据分配给每个工作节点，以便同时处理。\n如果模型大小大于单个 GPU 节点的内存，那么简单的 DP 就无法很好地工作。例如 GeePS 方法（Cui 等人，2016 ）将暂时未用的参数卸载回 CPU，以便在模型太大以至于无法装入一台机器时，仍能在有限的 GPU 内存下工作。数据交换传输应在后端进行，以免干扰训练计算。\n在每个小批量（minibatch）结束时，工作节点需要同步梯度或权重，以避免陈旧。有两种主要的同步方法，每种都有明显的优缺点。\n批量同步并行（BSP）：工作节点在每个小批量结束时同步数据。它可以防止模型权重的陈旧，并保持良好的学习效率，但每台机器都必须暂停并等待其他机器发送梯度。 异步并行（ASP）：每个 GPU 工作节点异步处理数据，无需等待或停滞。然而，它可能会导致使用陈旧的权重，从而降低统计学习效率。即使它增加了计算时间，也可能无法加速训练时间收敛。 中间的某个位置是在每隔 $x$ 次迭代（$x \u003e 1$）时全局同步梯度。从 Pytorch v1.5 开始，这个功能在分布式数据并行（DDP ）中被称为“梯度累积”（Li 等人，2021 ）。桶化梯度避免了立即进行 AllReduce 操作，而是将多个梯度合并成一个 AllReduce 以提高吞吐量。可以根据计算图对计算和通信调度进行优化。\n图 1. Pytorch DDP 的伪代码（图片来源：Li 等人，2021 ）\n模型并行 **模型并行（MP）**旨在解决模型权重无法装入单个节点的情况。计算和模型参数被分布在多台机器上。这与数据并行不同，数据并行中每个工作节点都有整个模型的完整副本，而 MP 只在一个工作节点上分配一部分模型参数，因此内存使用和计算都得到了减少。\n由于深度神经网络通常包含一堆垂直层，因此将大型模型按层分割，将一小组连续层组合成一个分区在一个工作节点上，看似很直接。然而，天真的实现通过多个这样的工作节点运行每个数据批次，而具有顺序依赖性，会导致大量等待时间的空泡，并严重低效利用计算资源。\n图 2. 一个天真的模型并行设置，其中模型垂直分为 4 个分区。由于顺序依赖性，数据由一个工作节点一次处理，导致大量的空闲时间“泡泡”。(图片来源：Huang et al. 2019 )\n管道并行 **管道并行（PP）**将模型并行与数据并行结合，以减少效率低下的时间“泡泡”。主要思想是将一个小批量分割成多个微批次，并使每个阶段的工作节点能同时处理一个微批次。注意，每个微批次需要两个传递，一个前向和一个后向。工作节点之间的通信只传输激活值（前向）和梯度（后向）。这些传递如何被调度，以及梯度如何被聚合，在不同的方法中有所不同。分区（工作节点）的数量也被称为管道深度。\n在 GPipe (Huang et al. 2019 ) 中，多个微批次的梯度被聚合并在最后同步应用。同步梯度下降保证了学习的一致性和效率，而不考虑工作节点的数量。如图 3 所示，仍然存在泡泡，但比图 2 中的要小得多。给定 $m$ 个均匀分割的微批次和 $d$ 个分区，假设每个微批次的前向和后向都需要一单位时间，泡泡的分数是：\n$$ 1 - \\frac{2md}{(2m + 2(d-1))d} = \\frac{d-1}{m+d-1} $$\nGPipe 论文观察到，如果微批处理的数量超过分区数量的 4 倍，即 $m \u003e 4d$（当应用激活重计算 时），泡沫开销几乎可以忽略不计。\n图 3. GPipe 中带有 4 个微批处理和 4 个分区的流水线并行性示意图。GPipe 在每个批处理结束时，跨设备同步地聚合并更新梯度。（图片来源：Huang et al. 2019 ）\n尽管如果模型参数没有在工作人员之间均匀分布，不能总是保证，但 GPipe 几乎可以实现与设备数量成线性比例的吞吐量增加。\nPipeDream（Narayanan et al. 2019 ）安排每个工作人员交替处理前向和反向传递（1F1B）。PipeDream 将每个模型分区命名为“阶段”，每个阶段工作人员可以有多个副本来运行数据并行性。在此过程中，PipeDream 使用确定性循环负载均衡策略在多个阶段的副本之间分配工作，以确保对同一小批量的前向和反向传递发生在同一副本上。\n图 4. PipeDream 中的 1F1B 微批处理调度示意图。（图片来源：Harlap et al. 2018 ）\n由于 PipeDream 不具有跨所有工作人员的批处理结束全局梯度同步，1F1B 的原生实现可能会导致一个微批处理的前向和反向传递使用不同版本的模型权重，从而降低学习效率。PipeDream 提出了几种设计来解决此问题：\n权重缓存：每个工作人员跟踪几个模型版本，并确保在给定一个数据批处理时，前向和反向传递使用相同版本的权重。 垂直同步（可选）：模型权重的版本与激活和梯度一起在阶段工作人员之间流动。然后，计算采用从前一个工作人员传播的相应缓存版本。此过程保持了工作人员之间的版本一致性。请注意，它是异步的，与 GPipe 不同。 在训练运行开始时，PipeDream 首先分析模型中每层的计算内存成本和时间，然后优化将层分区到阶段的解决方案，这是一个动态编程问题。\n图 5. VGG16 在 ILSVRC12 上的结果。（上）准确性与时间。整数标记阶段工作人员的数量。ASP = 异步并行 \u0026 BSP = 批量同步并行。（下）不同并行配置的训练时间加速。直线管道指的是没有数据并行性的管道并行性。（图片来源：Harlap et al. 2018 ）\n后来提出了两种 PipeDream 变体，以减少通过缓存模型版本的内存占用（Narayanan et al. 2021 ）。\nPipeDream-flush 定期添加了全局同步的管道刷新，就像 GPipe 一样。通过这种方式，它大大减少了内存占用（即只保留一个模型权重版本），牺牲了一点吞吐量。\n图 6. PipeDream-flush 中的管道调度示意图。（图片来源：(Narayanan et al. 2021 )\nPipeDream-2BW 只保留两个版本的模型权重，其中 “2BW” 是 “双缓冲权重” 的缩写。它每隔 $k$ 微批处理生成一个新的模型版本，$k$ 应大于管道深度 $d$，即 $k \u003e d$。新更新的模型版本不能立即完全替换旧版本，因为一些剩余的反向传递仍然依赖于旧版本。总的来说，只需保存两个版本，因此内存成本大大减少。\n图 7. PipeDream-2BW 中的管道调度示意图。（图片来源： (Narayanan et al. 2021 )\n张量并行性 模型并行和流水线并行都是垂直切分模型。但另一方面，我们可以将一个张量运算的计算过程横向切分到多个设备上，这被称为 张量并行性（TP）。\n以现在广泛使用的 Transformer 为例。Transformer 模型主要由多层 MLP 和自注意力模块组成。Megatron-LM (Shoeybi 等人，2020 )采用了一种简单的方法来并行化 MLP 和自注意力层内的计算。\nTransformer 中的 MLP 层包含了一个 GEMM（通用矩阵乘法）操作，接着是一个非线性的 GeLU 传递函数。我们按列切分权重矩阵 $A$：\n$$ \\begin{aligned} \\text{切分 } A \u0026= [A_1, A_2] \\ Y \u0026= \\text{GeLU}(XA) \\ [Y_1, Y_2] \u0026= [\\text{GeLU}(XA_1), \\text{GeLU}(XA_2)] \\end{aligned} $$\n注意力模块按照上述切分方式并行运行 GEMM，对 query（$Q$）、key（$K$）和 value（$V$）权重进行计算，然后再通过另一个 GEMM 将它们组合起来，以产生注意力头结果。\n$$ \\text{Attention}(X, Q, K, V) = \\text{softmax}\\left(\\frac{(XQ) (XK)^\\top}{\\sqrt{d\\_k}}\\right) XV $$ 图 8. Megatron-LM 中提出的针对关键 Transformer 组件的张量并行性示意图。（图片来源：Shoeybi 等人，2020 ）\nNarayanan 等人（2021） 将流水线、张量和数据并行结合了新的流水线调度策略，将其命名为 PTD-P。与仅将连续层集（“模型块”）放置在一个设备上不同，每个工作器都可以被分配多个较小的连续层子集的块（例如，设备 1 有层 1、2、9、10；设备 2 有层 3、4、11、12；每个设备都有两个模型块）。一个批次中的微批次数量应该能够被工作器数量（$m % d = 0$）整除。如果每个工作器有 $v$ 个模型块，与 GPipe 调度相比，流水线泡沫时间可以减少 $v$ 倍。\n图 9.（顶部）默认的 1F1B 流水线调度，如 PipeDream-flush 中所示。（底部）交错的 1F1B 流水线调度。第一模型块颜色较深，第二模型块颜色较浅。（图片来源：Narayanan 等人，2021 ）\n专家混合 (MoE) 近期，专家混合 (Mixture-of-Experts, MoE) 方法吸引了许多关注，研究人员（主要来自 Google）尝试着推动模型大小的极限。这个想法的核心是集成学习 ：多个弱学习器的组合可以产生一个强学习器！\n在一个深度神经网络中，可以通过与多个专家连接的门控机制来实现集成（Shazeer et al., 2017 ）。门控机制控制着网络的哪个子集（例如，哪些专家）应该被激活以产生输出。该论文将其命名为“稀疏门控专家混合” (MoE) 层。\n准确地说，一个 MoE 层包含\n$n$ 个前馈网络作为专家 ${E_i}^n_{i=1}$ 一个可训练的门控网络 $G$，用于学习 $n$ 个专家上的概率分布，以便将流量路由到一些选定的专家。 根据门控输出，不是每个专家都必须被评估。当专家数量太大时，我们可以考虑使用两级层次的 MoE。\n图 10. 专家混合（MoE）层的示意图。门控网络选中并激活了 $n$ 个专家中的 2 个。（图片来源：Shazeer et al., 2017 ）\n一个简单的 $G$ 选择是将输入与一个可训练的权重矩阵 $G_g$ 相乘，然后进行 softmax：$G_\\sigma(x) = \\text{softmax}(x W_g)$。然而，这会产生一个密集的控制向量用于门控，并且不能节省计算资源，因为我们只有当 $G^{(i)}(x)=0$ 时才不需要评估一个专家。因此 MoE 层只保留了前 $k$ 个值。它还将可调的高斯噪声添加到 $G$ 中以改善负载平衡。这个机制被称为 噪音 top-k 门控。\n$$ \\begin{aligned} G(x) \u0026= \\text{softmax}(\\text{topk}(H(x), k)) \\ H^{(i)}(x) \u0026= (xW_g)^{(i)} + \\epsilon \\cdot \\text{softplus}((xW_\\text{noise})^{(i)} ); \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{1}) \\ \\text{topk}^{(i)}(v, k) \u0026= \\begin{cases} v^{(i)} \u0026 \\text{if }v^{(i)}\\text{ is in the top }k\\text{ elements of }v \\ -\\infty \u0026 \\text{otherwise} \\end{cases} \\end{aligned} $$\n其中上标 $v^{(i)}$ 表示向量 $v$ 的第 i 维。函数 $\\text{topk}(., k)$ 通过将其他维度设置为 $-\\infty$ 来选择最高值的前 $k$ 维。\n为了避免门控网络可能一直偏爱几个强专家的自强化效应，Shazeer et al. (2017) 提出了通过额外的重要性损失来鼓励所有专家具有相同权重的软约束。它相当于每个专家的批量平均值的变异系数 的平方。\n$$ L_\\text{aux} = w_\\text{aux} \\cdot \\text{CV}(\\sum_{x \\in X} G(x))^2 $$\n其中 $ \\text{CV}$ 是变异系数，损失权重 $w_\\text{aux}$ 是一个要调整的超参数。\n由于每个专家网络只能获得一部分训练样本（“缩小的批处理问题”），我们应该尽量使用尽可能大的批大小在 MoE 中。然而，它受到 GPU 内存的限制。可以应用数据并行和模型并行来提高吞吐量。\n图 11. 在 1-Billion-Word 语言建模基准测试上的困惑度测试。（左图）模型容量从左到右递增，包含 4、32、256、256、1024 和 4096 个专家。（右图）在不同计算预算下，4 十亿参数的 MoE 模型的性能，这是左图中最大的一个模型。（图片来源：Shazeer et al., 2017 ）\nGShard (Lepikhin et al., 2020 ) 通过切片技术，将 MoE 变换器模型的参数量扩展到 6000 亿。MoE 变换器将每隔一个前馈层替换为 MoE 层。切片的 MoE 变换器 只有 MoE 层是跨多台机器切片的，而其他层则简单地复制了。\nGShard 中对门控函数 $ G $ 的几个改进设计包括：\n专家容量：通过一个专家的令牌数量不应超过一个名为“专家容量”的阈值。如果一个令牌被路由到已经达到其容量的专家，该令牌将被标记为“溢出”，并且门控输出将变为零向量。 本地组调度：令牌被均匀划分为多个本地组，并在组级别上执行专家容量。 辅助损失：其动机与原始 MoE 辅助损失相似。他们添加了一个辅助损失，以最小化路由到每个专家的数据分数的均方。 随机路由：第二好的专家被选中的概率与其权重成正比；否则，GShard 遵循随机路由，以增加一些随机性。 图 12. GShard 中带有辅助损失的组级别 top-2 门控机制的伪代码。（图片来源：Lepikhin et al., 2020 ）\nSwitch Transformer (Fedus et al. 2021 ) 通过将密集前馈层替换为 稀疏开关 FFN 层，将模型大小扩展到数万亿参数(!!)，在该层中，每个输入只路由到 一个 专家网络。负载平衡的辅助损失为 $ \\text{loss}_{\\text{aux}} = w_{\\text{aux}} \\sum_{i=1}^n f_i p_i $，其中 $ n $ 是专家的数量，$ f_i $ 是路由到第 $ i $ 个专家的令牌分数，$ p_i $ 是由门控网络预测的第 $ i $ 个专家的路由概率。\n图 13. Switch Transformer。稀疏开关 FFN 层位于蓝色框内。（图片来源：Fedus et al. 2021 ）\n为了提高训练稳定性，Switch Transformer 采用了以下设计：\n选择性精度。他们表明，只将模型的局部部分选择性地转换为 FP32 精度可以提高稳定性，同时避免了 FP32 张量的昂贵通信成本。FP32 精度仅用于路由器函数的主体内，结果重新转换为 FP16。 较小的初始化。权重矩阵的初始化是从截断正态分布中采样的，均值 $ \\mu = 0 $，标准差 $ \\sigma = \\sqrt{s/n} $。他们还建议将变换器初始化比例参数 $ s=1 $ 减小到 $ s=0.1 $。 使用更高的专家退出率。微调通常适用于小数据集。为了避免过拟合，他们在每个专家内大幅增加了退出率。有趣的是，他们发现在所有层增加退出率会导致性能下降。在论文中，他们在非专家层使用了 0.1 的退出率，但在专家 FF 层内使用了 0.4 的退出率。 Switch Transformer 论文总结了用于训练大型模型的不同数据和模型并行策略，并提供了一个很好的示意图：\n图 14. 关于如何（顶部）模型权重和（底部）数据在多个 GPU 核心上分割的各种并行策略的示意图。在顶行，每种颜色代表一个唯一的权重矩阵。在底行，不同的颜色表示不同的令牌集。 (图片来源：Fedus et al. 2021 )\nGShard top-2\r和 Switch Transformer top-1 都依赖于 令牌选择，其中每个令牌选择最好的一个或两个专家来进行路由。它们都采用了一个辅助损失来鼓励更均衡的负载分配，但不能保证最佳性能。此外，专家的容量限制可能会导致令牌的浪费，因为如果一个专家达到其容量限制，令牌将被丢弃。\n专家选择 (EC) (Zhou et al. 2022 ) 的路由使每个专家能够选择 top-$k$ 令牌。这样，每个专家自然地保证了固定的容量，每个令牌可能会被路由到多个专家。EC 能够实现完美的负载平衡，并且已显示出能够将训练收敛速度提高 2 倍。\n给定 $e$ 个专家和一个输入矩阵 $X \\in \\mathbb{R}^{n \\times d}$，通过计算得到令牌到专家的亲和得分：$$ S = \\text{softmax}(X \\cdot W_g), \\text{其中 } W_g \\in \\mathbb{R}^{d \\times e}, S \\in \\mathbb{R}^{n \\times e} $$\n令牌到专家的分配由三个矩阵表示，$I, G \\in \\mathbb{R}^{e\\times k}$ 和 $P \\in \\mathbb{R}^{e \\times k \\times n}$。$I[i,j]$ 标注了由第 $i$ 个专家选择的第 $j$ 个令牌。门控矩阵 $G$ 存储了选定令牌的路由权重。$P$ 是 $I$ 的 one-hot 版本，用于产生门控 FFN 层的输入矩阵 ($P \\cdot X \\in \\mathbb{R}^{e \\times k \\times d}$)。$$ G, I = \\text{top-k}(S^\\top, k) \\quad P = \\text{one-hot}(I) $$\n专家选择路由探索的一个正则化是限制每个令牌的最大专家数量。\n$$ \\begin{aligned} \u0026 \\max_A \\langle S^\\top, A\\rangle + \\lambda H(A) \\\\ \\text{s.t.} \u0026 \\forall i: \\sum_{j’} A[i, j’] = k, \\quad \\forall j: \\sum_{i’} A[i’, j] \\leq b, \\quad \\forall i,j: 0 \\leq A[i,j] \\leq 1 \\end{aligned} $$\n其中，每个条目 $A[i,j]$ 在 $A \\in \\mathbb{R}^{e \\times n}$ 中标记第 $i$ 个专家是否选择第 $j$ 个令牌。解决这个问题并非易事。论文使用了 Dykstra 的算法 进行了一系列多次迭代计算步骤。在实验中，限制专家选择导致微调性能略有下降。\n参数 $k$ 由 $k=nc/e$ 确定，其中 $n$ 是一个批次中的总令牌数，$c$ 是表示一个令牌平均使用的专家数量的容量因子。论文在大多数实验中使用了 $c=2$，但是 $c=1$ 的 EC 仍然优于 top-1 令牌选择门控。有趣的是，$c=0.5$ 仅对训练性能造成了边际损害。\nEC 的一个大缺点是当批大小太小时，它不起作用，也不适用于自回归文本生成，因为它需要知道未来的令牌来进行 top-$k$ 选择。\n$$ \\begin{aligned} \u0026 \\max_A \\langle S^\\top, A\\rangle + \\lambda H(A) \\\\ \\text{s.t.} \u0026 \\forall i: \\sum_{j'} A[i, j'] = k,\\quad \\forall j: \\sum_{i'} A[i', j] \\leq b,\\quad \\forall i,j: 0 \\leq A[i,j] \\leq 1 \\end{aligned} $$\n每个条目 $A[i,j]$ 标记在 $A \\in \\mathbb{R}^{e \\times n}$ 中。是否 $i$-位专家选择了 $j$-位令牌。解决这个问题并非易事。该论文使用了 Dykstra 的算法 ，它通过运行多个迭代计算步骤的序列来进行。在实验中，限制专家选择会轻微降低微调性能。\n参数 $k$ 由 $k=nc/e$ 确定，其中 $n$ 是一个批次中的令牌总数，$c$ 是一个表示由一个令牌使用的专家平均数的容量因子。在大多数实验中，论文使用了 $c=2$，但是即使 $c=1$，EC 仍然优于顶部 -1 令牌选择门控。有趣的是，$c = 0.5$ 只会对训练性能造成微小的伤害。\nEC 的一个大缺点是，当批量太小时它不起作用，对于自回归文本生成也是如此，因为它需要知道未来的令牌来进行顶部-$k$ 选择。\n其他节省内存的设计 CPU 卸载 当 GPU 内存已满时，一种选择是暂时将未使用的数据卸载到 CPU，并在稍后需要时读回它们（Rhu 等人，2016 ）。CPU 卸载的想法很简单，但由于它使训练时间变慢，所以近年来较为不流行。\n激活重计算 激活重计算（也称为 “激活检查点” 或 “梯度检查点”；Chen 等人，2016 ）是一种聪明而简单的想法，它以计算时间为代价来减少内存占用。它将训练一个 $ℓ$ 层深神经网络的内存成本降低到 $O(√ℓ)$，每个批次只额外消耗一个前向传递计算。\n假设我们将一个 $ℓ$-层网络均匀划分为 $d$ 个分区。只有在分区边界处的激活被保存和在工作者之间传输。在分区内的层上的中间激活仍然需要计算梯度，所以它们在反向传递期间被重新计算。使用激活重计算，训练 $M(ℓ)$ 的内存成本为：\n$$[ M(ℓ) =\\max_{i=1,\\dots,k} \\underbrace{\\text{cost-of-one-partition}(i)}_{\\text{cost of back-propagation on the i-th partition}} + \\underbrace{O(d)}_{\\text{store intermediate outputs}} = O(\\frac{ℓ}{d}) + O(d) ] $$\n在 $d=\\sqrt{\\ell}$ 处的最小成本是 $O(\\sqrt{\\ell})$。\n激活重计算技巧可以使内存成本相对于模型大小呈亚线性。\n图 15. 不同节省内存算法的内存成本。 [Sharing]{.ul}：在不再需要时，由中间结果使用的内存被回收。 [Inplace]{.ul}：直接将输出保存到输入值的内存中。 (图片来源：Chen 等人，2016 )\n混合精度训练 Narang \u0026 Micikevicius 等人 (2018) 提出了一种使用半精度浮点数（FP16）训练模型而不损失模型准确性的方法。\n图 16. 一层混合精度训练的过程。（图片来源：Narang \u0026 Micikevicius 等人 2018 ）\n为避免在半精度下丢失关键信息的三种技术：\n全精度主权重副本。保持模型权重的全精度（FP32）副本，以累积梯度。这些数字会在前向和后向传递时四舍五入到半精度。动机是每次梯度更新（即，梯度乘以学习率）可能太小，无法完全包含在 FP16 范围内（例如，$2^{-24}$ 在 FP16 中变为零）。 损失缩放。放大损失以更好地处理具有小幅度的梯度（参见图 16）。放大梯度有助于将它们移至可表示范围的右侧（包含较大值）的更大部分，以保留否则会丢失的值。 算术精度。对于常见的网络算术（例如，向量点积，通过求和向量元素进行缩减），我们可以在 FP32 中累积部分结果，然后在保存到内存之前将最终输出保存为 FP16。点式操作可以在 FP16 或 FP32 中执行。 图 17. 全精度梯度的直方图。一旦模型切换到 FP16，直至 $2^{-24}$ 的左侧部分将被清零。（图片来源：Narang \u0026 Micikevicius 等人 2018 ）\n在他们的实验中，对于某些网络（例如，图像分类，Faster R-CNN）不需要损失缩放，但对于其他网络（例如，Multibox SSD，大型 LSTM 语言模型）则是必需的。\n压缩 中间结果通常会消耗大量内存，尽管它们只在一个前向传递和一个后向传递中需要。在这两种使用之间存在明显的时间间隔。因此 Jain 等人 (2018) 提出了一种数据编码策略，以在第一次传递的第一次使用后压缩中间结果，然后在稍后的反向传播中对其进行解码。\n他们的系统 Gist 包含两种编码方案：特定于层的无损编码；重点关注 ReLU-Pool（“二值化”）和 ReLU-Conv（“稀疏存储和密集计算”）模式。积极的有损编码；使用延迟精度降低（DPR）。他们观察到，特征图的第一次直接使用应保持高精度，但第二次使用可以容忍较低的精度。\n实验表明，Gist 可以在 5 个 SOTA 图像分类 DNN 中将内存成本减少 2 倍，平均仅有 4% 的性能开销，平均减少 1.8 倍。\n内存高效优化器 优化器对内存的需求很高。以流行的 Adam 优化器为例，它内部需要维护动量和方差，其规模与梯度和模型参数相同。突然间，我们需要保存模型权重的 4 倍内存。\n为了减少内存占用，已经提出了一些优化器。例如，与 Adam 中存储完整动量和变化不同，Adafactor（Shazeer 等人，2018 ）只跟踪移动平均值的每行和每列总和，然后根据这些总和估算第二时刻。SM3（Anil 等人，2019 ）描述了一种不同的自适应优化方法，也大大减少了内存占用。\nZeRO（Zero Redundancy Optimizer，Rajbhandari 等人，2019 ）优化了用于训练大型模型的内存使用，基于对大型模型训练的两个主要内存消耗的观察：\n大部分被 模型状态 占用，包括优化器状态（例如 Adam 的动量和方差）、梯度和参数。混合精度训练需要大量内存，因为优化器需要保留 FP32 参数和其他优化器状态的副本，除了 FP16 版本。 其余的被激活、临时缓冲区和不可用的碎片化内存（在论文中称为 残余状态）消耗。 ZeRO 结合了两种方法，ZeRO-DP 和 ZeRO-R。ZeRO-DP 是一种增强的数据并行性，以避免模型状态上的简单冗余。它通过动态通信时间表将优化器状态、梯度和参数分区到多个数据并行过程中，以最小化通信量。ZeRO-R 优化了残余状态的内存消耗，使用分区激活重计算、常量缓冲区大小和实时内存碎片整理。\n引用 Cited as:\nWeng, Lilian. (Sep 2021). How to train really large models on many GPUs? Lil’Log. https://lilianweng.github.io/posts/2021-09-25-train-large/ .\nOr\n1 2 3 4 5 6 7 8 @article{weng2021large, title = \"How to Train Really Large Models on Many GPUs?\", author = \"Weng, Lilian\", journal = \"lilianweng.github.io\", year = \"2021\", month = \"Sep\", url = \"https://lilianweng.github.io/posts/2021-09-25-train-large/\" } 参考文献 [1] Li et al. “PyTorch Distributed: Experiences on Accelerating Data Parallel Training” VLDB 2020.\n[2] Cui et al. “GeePS: Scalable deep learning on distributed GPUs with a GPU-specialized parameter server” EuroSys 2016\n[3] Shoeybi et al. “Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism.” arXiv preprint arXiv:1909.08053 (2019).\n[4] Narayanan et al. “Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM.” arXiv preprint arXiv:2104.04473 (2021).\n[5] Huang et al. “GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism.” arXiv preprint arXiv:1811.06965 (2018).\n[6] Narayanan et al. “PipeDream: Generalized Pipeline Parallelism for DNN Training.\" SOSP 2019.\n[7] Narayanan et al. “Memory-Efficient Pipeline-Parallel DNN Training.” ICML 2021.\n[8] Shazeer et al. “The Sparsely-Gated Mixture-of-Experts Layer Noam.” arXiv preprint arXiv:1701.06538 (2017).\n[9] Lepikhin et al. “GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding.” arXiv preprint arXiv:2006.16668 (2020).\n[10] Fedus et al. “Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.” arXiv preprint arXiv:2101.03961 (2021).\n[11] Narang \u0026 Micikevicius, et al. “Mixed precision training.” ICLR 2018.\n[12] Chen et al. 2016 “Training Deep Nets with Sublinear Memory Cost.” arXiv preprint arXiv:1604.06174 (2016).\n[13] Jain et al. “Gist: Efficient data encoding for deep neural network training.” ISCA 2018.\n[14] Shazeer \u0026 Stern. “Adafactor: Adaptive learning rates with sublinear memory cost.” arXiv preprint arXiv:1804.04235 (2018).\n[15] Anil et al. “Memory-Efficient Adaptive Optimization.” arXiv preprint arXiv:1901.11150 (2019).\n[16] Rajbhandari et al. “ZeRO: Memory Optimization Towards Training A Trillion Parameter Models Samyam.” arXiv preprint arXiv:1910.02054 (2019).\n[17] Zhou et al. “Mixture-of-Experts with Expert Choice Routing” arXiv preprint arXiv:2202.09368 (2022).\narchitecture transformer foundation ",
  "wordCount" : "8582",
  "inLanguage": "en",
  "datePublished": "2023-10-06T03:03:40+08:00",
  "dateModified": "2023-10-06T03:03:40+08:00",
  "author":{
    "@type": "Person",
    "name": "Lilian Weng"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://awyugan.github.io/2023/10/06/%E8%AF%91%E5%A6%82%E4%BD%95%E5%9C%A8%E5%A4%9A-gpu-%E4%B8%8A%E8%AE%AD%E7%BB%83%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%A4%A7%E5%9E%8B%E6%A8%A1%E5%9E%8B-lillog/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Awyugan's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://awyugan.github.io/img/Q.gif"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://awyugan.github.io" accesskey="h" title="Awyugan&#39;s Blog (Alt + H)">
                <img src="https://awyugan.github.io/img/avatar.jpg" alt="" aria-label="logo"
                    height="35">Awyugan&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://awyugan.github.io/search/" title="Search">
                    <span>🔍 搜索</span>
                </a>
            </li>
            <li>
                <a href="https://awyugan.github.io/" title="Awyugan&#39;s Blog">
                    <span>🏠 主页</span>
                </a>
            </li>
            <li>
                <a href="https://awyugan.github.io/archives/" title="Archives">
                    <span>🗃️ 归档</span>
                </a>
            </li>
            <li>
                <a href="https://awyugan.github.io/tags/" title="Tags">
                    <span>🔖 标签</span>
                </a>
            </li>
            <li>
                <a href="https://awyugan.github.io/about/" title="关于我">
                    <span>ℹ️ 关于</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://awyugan.github.io">Home</a>&nbsp;»&nbsp;<a href="https://awyugan.github.io/post/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log
    </h1>
    <div class="post-meta">










创建:&nbsp;<span title='2023-10-06 03:03:40 +0800 CST'>2023-10-06</span>&nbsp;·&nbsp;更新:&nbsp;2023-10-06&nbsp;·&nbsp;字数:&nbsp;8582字&nbsp;·&nbsp;时长: 18分钟&nbsp;·&nbsp;
作者:&nbsp;Lilian Weng

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e8%ae%ad%e7%bb%83%e5%b9%b6%e8%a1%8c%e6%80%a7" aria-label="训练并行性">训练并行性</a><ul>
                        
                <li>
                    <a href="#%e6%95%b0%e6%8d%ae%e5%b9%b6%e8%a1%8c" aria-label="数据并行">数据并行</a></li>
                <li>
                    <a href="#%e6%a8%a1%e5%9e%8b%e5%b9%b6%e8%a1%8c" aria-label="模型并行">模型并行</a></li>
                <li>
                    <a href="#%e7%ae%a1%e9%81%93%e5%b9%b6%e8%a1%8c" aria-label="管道并行">管道并行</a></li>
                <li>
                    <a href="#%e5%bc%a0%e9%87%8f%e5%b9%b6%e8%a1%8c%e6%80%a7" aria-label="张量并行性">张量并行性</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%b8%93%e5%ae%b6%e6%b7%b7%e5%90%88-moe" aria-label="专家混合 (MoE)">专家混合 (MoE)</a></li>
                <li>
                    <a href="#%e5%85%b6%e4%bb%96%e8%8a%82%e7%9c%81%e5%86%85%e5%ad%98%e7%9a%84%e8%ae%be%e8%ae%a1" aria-label="其他节省内存的设计">其他节省内存的设计</a><ul>
                        
                <li>
                    <a href="#cpu-%e5%8d%b8%e8%bd%bd" aria-label="CPU 卸载">CPU 卸载</a></li>
                <li>
                    <a href="#%e6%bf%80%e6%b4%bb%e9%87%8d%e8%ae%a1%e7%ae%97" aria-label="激活重计算">激活重计算</a></li>
                <li>
                    <a href="#%e6%b7%b7%e5%90%88%e7%b2%be%e5%ba%a6%e8%ae%ad%e7%bb%83" aria-label="混合精度训练">混合精度训练</a></li>
                <li>
                    <a href="#%e5%8e%8b%e7%bc%a9" aria-label="压缩">压缩</a></li>
                <li>
                    <a href="#%e5%86%85%e5%ad%98%e9%ab%98%e6%95%88%e4%bc%98%e5%8c%96%e5%99%a8" aria-label="内存高效优化器">内存高效优化器</a></li>
                <li>
                    <a href="#%e5%bc%95%e7%94%a8" aria-label="引用">引用</a></li>
                <li>
                    <a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" aria-label="参考文献">参考文献</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>2022-03-13 更新: 添加 <a href="#ec">expert choice routing</a>
</p>
<p>2022-06-10 更新: <a href="https://gregbrockman.com/" target="_blank" rel="noopener">Greg</a>
 和我撰写了这篇文章的缩短和升级版本，发表在 OpenAI Blog 上：<a href="https://openai.com/blog/techniques-for-training-large-neural-networks/" target="_blank" rel="noopener">&ldquo;训练大型神经网络的技术&rdquo;</a>
</p>
<p>近年来，我们在许多 NLP 基准任务上看到，更大的预训练<a href="https://lilianweng.github.io/posts/2019-01-31-lm/" target="_blank" rel="noopener">语言模型</a>
能够取得更好的结果。但是，训练大型和深层神经网络具有挑战性，因为它需要大量的 GPU 内存和长时间的训练周期。</p>
<p>然而，单个 GPU 工作器的内存是有限的，许多大型模型的大小已经超过了单个 GPU 的容量。有几种并行计算范式可以实现跨多个 GPU 的模型训练，还有各种模型架构和内存节省设计，以帮助训练<em>非常大</em>的神经网络成为可能。</p>
<h1 id="训练并行性">训练并行性<a hidden class="anchor" aria-hidden="true" href="#训练并行性">#</a></h1>
<p>训练非常大的神经网络模型的主要瓶颈是对大量 GPU 内存的强烈需求，远远超过单个 GPU 机器所能承载的。除了模型权重（例如，数百亿的浮点数）之外，通常存储中间计算输出（如梯度和优化器状态（例如，Adam 中的动量和变异））的成本更高。此外，训练大型模型通常需要配备大型训练语料库，因此单个过程可能需要很长时间。</p>
<p>因此，必须采用并行计算。并行计算可以在不同维度进行，包括数据、模型架构和张量运算。</p>
<h2 id="数据并行">数据并行<a hidden class="anchor" aria-hidden="true" href="#数据并行">#</a></h2>
<p>最简单的 <strong>数据并行（DP）</strong> 方法是将相同的模型权重复制到多个工作节点，并将一部分数据分配给每个工作节点，以便同时处理。</p>
<p>如果模型大小大于单个 GPU 节点的内存，那么简单的 DP 就无法很好地工作。例如 <em>GeePS</em> 方法（<a href="https://www.pdl.cmu.edu/PDL-FTP/CloudComputing/GeePS-cui-eurosys16.pdf" target="_blank" rel="noopener">Cui 等人，2016</a>
）将暂时未用的参数卸载回 CPU，以便在模型太大以至于无法装入一台机器时，仍能在有限的 GPU 内存下工作。数据交换传输应在后端进行，以免干扰训练计算。</p>
<p>在每个小批量（minibatch）结束时，工作节点需要同步梯度或权重，以避免陈旧。有两种主要的同步方法，每种都有明显的优缺点。</p>
<ol>
<li><em>批量同步并行（BSP）</em>：工作节点在每个小批量结束时同步数据。它可以防止模型权重的陈旧，并保持良好的学习效率，但每台机器都必须暂停并等待其他机器发送梯度。</li>
<li><em>异步并行（ASP）</em>：每个 GPU 工作节点异步处理数据，无需等待或停滞。然而，它可能会导致使用陈旧的权重，从而降低统计学习效率。即使它增加了计算时间，也可能无法加速训练时间收敛。</li>
</ol>
<p>中间的某个位置是在每隔 $x$ 次迭代（$x &gt; 1$）时全局同步梯度。从 Pytorch v1.5 开始，这个功能在分布式数据并行（<a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" target="_blank" rel="noopener">DDP</a>
）中被称为“梯度累积”（<a href="https://arxiv.org/abs/2006.15704" target="_blank" rel="noopener">Li 等人，2021</a>
）。桶化梯度避免了立即进行 <code>AllReduce</code> 操作，而是将多个梯度合并成一个 <code>AllReduce</code> 以提高吞吐量。可以根据计算图对计算和通信调度进行优化。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/nrkpnXFyaXSoBrumuZk185"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/nrkpnXFyaXSoBrumuZk185" alt="pytorch-ddp.png" border="0" /></a></p>
<p>图 1. Pytorch DDP 的伪代码（图片来源：<a href="https://arxiv.org/abs/2006.15704" target="_blank" rel="noopener">Li 等人，2021</a>
）</p>
<h2 id="模型并行">模型并行<a hidden class="anchor" aria-hidden="true" href="#模型并行">#</a></h2>
<p>**模型并行（MP）**旨在解决模型权重无法装入单个节点的情况。计算和模型参数被分布在多台机器上。这与数据并行不同，数据并行中每个工作节点都有整个模型的完整副本，而 MP 只在一个工作节点上分配一部分模型参数，因此内存使用和计算都得到了减少。</p>
<p>由于深度神经网络通常包含一堆垂直层，因此将大型模型按层分割，将一小组连续层组合成一个分区在一个工作节点上，看似很直接。然而，天真的实现通过多个这样的工作节点运行每个数据批次，而具有顺序依赖性，会导致大量等待时间的空泡，并严重低效利用计算资源。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/qbmURo9UXaXPLEjHtrZwWc"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/qbmURo9UXaXPLEjHtrZwWc" alt="naive-data-parallelism.png" border="0" /></a></p>
<p>图 2. 一个天真的模型并行设置，其中模型垂直分为 4 个分区。由于顺序依赖性，数据由一个工作节点一次处理，导致大量的空闲时间“泡泡”。(图片来源：<a href="https://arxiv.org/abs/1811.06965" target="_blank" rel="noopener">Huang et al. 2019</a>
)</p>
<h2 id="管道并行">管道并行<a hidden class="anchor" aria-hidden="true" href="#管道并行">#</a></h2>
<p>**管道并行（PP）**将模型并行与数据并行结合，以减少效率低下的时间“泡泡”。主要思想是将一个小批量分割成多个微批次，并使每个阶段的工作节点能同时处理一个微批次。注意，每个微批次需要两个传递，一个前向和一个后向。工作节点之间的通信只传输激活值（前向）和梯度（后向）。这些传递如何被调度，以及梯度如何被聚合，在不同的方法中有所不同。分区（工作节点）的数量也被称为<em>管道深度</em>。</p>
<p>在 <em>GPipe</em> (<a href="https://arxiv.org/abs/1811.06965" target="_blank" rel="noopener">Huang et al. 2019</a>
) 中，多个微批次的梯度被聚合并在最后同步应用。同步梯度下降保证了学习的一致性和效率，而不考虑工作节点的数量。如图 3 所示，仍然存在泡泡，但比图 2 中的要小得多。给定 $m$ 个均匀分割的微批次和 $d$ 个分区，假设每个微批次的前向和后向都需要一单位时间，泡泡的分数是：</p>
<p>$$ 1 - \frac{2md}{(2m + 2(d-1))d} = \frac{d-1}{m+d-1} $$</p>
<p>GPipe 论文观察到，如果微批处理的数量超过分区数量的 4 倍，即 $m &gt; 4d$（当应用<a href="#%e6%bf%80%e6%b4%bb%e9%87%8d%e8%ae%a1%e7%ae%97">激活重计算</a>
时），泡沫开销几乎可以忽略不计。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/nJGfCsV3wLugPQHo53AA9y"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/nJGfCsV3wLugPQHo53AA9y" alt="gpipe.png" border="0" /></a></p>
<p>图 3. GPipe 中带有 4 个微批处理和 4 个分区的流水线并行性示意图。GPipe 在每个批处理结束时，跨设备同步地聚合并更新梯度。（图片来源：<a href="https://arxiv.org/abs/1811.06965" target="_blank" rel="noopener">Huang et al. 2019</a>
）</p>
<p>尽管如果模型参数没有在工作人员之间均匀分布，不能总是保证，但 GPipe 几乎可以实现与设备数量成线性比例的吞吐量增加。</p>
<p><em>PipeDream</em>（<a href="https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf" target="_blank" rel="noopener">Narayanan et al. 2019</a>
）安排每个工作人员交替处理前向和反向传递（<code>1F1B</code>）。PipeDream 将每个模型分区命名为“阶段”，每个阶段工作人员可以有多个副本来运行数据并行性。在此过程中，PipeDream 使用确定性循环负载均衡策略在多个阶段的副本之间分配工作，以确保对同一小批量的前向和反向传递发生在同一副本上。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/oozTi1f7TjQvkW3D9qUnqa"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/oozTi1f7TjQvkW3D9qUnqa" alt="pipedream.png" border="0" /></a></p>
<p>图 4. PipeDream 中的 <code>1F1B</code> 微批处理调度示意图。（图片来源：<a href="https://arxiv.org/abs/1806.03377" target="_blank" rel="noopener">Harlap et al. 2018</a>
）</p>
<p>由于 PipeDream 不具有跨所有工作人员的批处理结束全局梯度同步，1F1B 的原生实现可能会导致一个微批处理的前向和反向传递使用不同版本的模型权重，从而降低学习效率。PipeDream 提出了几种设计来解决此问题：</p>
<ul>
<li><em>权重缓存</em>：每个工作人员跟踪几个模型版本，并确保在给定一个数据批处理时，前向和反向传递使用相同版本的权重。</li>
<li><em>垂直同步</em>（可选）：模型权重的版本与激活和梯度一起在阶段工作人员之间流动。然后，计算采用从前一个工作人员传播的相应缓存版本。此过程保持了工作人员之间的版本一致性。请注意，它是异步的，与 GPipe 不同。</li>
</ul>
<p>在训练运行开始时，PipeDream 首先分析模型中每层的计算内存成本和时间，然后优化将层分区到阶段的解决方案，这是一个动态编程问题。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/dJPu1W6mrMX27bQo4xRFSp"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/dJPu1W6mrMX27bQo4xRFSp" alt="pipedream-results.png" border="0" /></a></p>
<p>图 5. VGG16 在 ILSVRC12 上的结果。（上）准确性与时间。整数标记阶段工作人员的数量。ASP = 异步并行 &amp; BSP = 批量同步并行。（下）不同并行配置的训练时间加速。直线管道指的是没有数据并行性的管道并行性。（图片来源：<a href="https://arxiv.org/abs/1806.03377" target="_blank" rel="noopener">Harlap et al. 2018</a>
）</p>
<p>后来提出了两种 PipeDream 变体，以减少通过缓存模型版本的内存占用（<a href="https://arxiv.org/abs/2006.09503" target="_blank" rel="noopener">Narayanan et al. 2021</a>
）。</p>
<p><em>PipeDream-flush</em> 定期添加了全局同步的管道刷新，就像 GPipe 一样。通过这种方式，它大大减少了内存占用（即只保留一个模型权重版本），牺牲了一点吞吐量。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/7zTdbAht9UYSA2QAjdxedh"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/7zTdbAht9UYSA2QAjdxedh" alt="pipedream-flush.png" border="0" /></a></p>
<p>图 6. PipeDream-flush 中的管道调度示意图。（图片来源：(<a href="https://arxiv.org/abs/2006.09503" target="_blank" rel="noopener">Narayanan et al. 2021</a>
)</p>
<p><em>PipeDream-2BW</em> 只保留两个版本的模型权重，其中 &ldquo;2BW&rdquo; 是 &ldquo;双缓冲权重&rdquo; 的缩写。它每隔 $k$ 微批处理生成一个新的模型版本，$k$ 应大于管道深度 $d$，即 $k &gt; d$。新更新的模型版本不能立即完全替换旧版本，因为一些剩余的反向传递仍然依赖于旧版本。总的来说，只需保存两个版本，因此内存成本大大减少。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/qeYfVcf5ZaiqJcJeiGp2kn"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/qeYfVcf5ZaiqJcJeiGp2kn" alt="pipedream-2bw.png" border="0" /></a></p>
<p>图 7. PipeDream-2BW 中的管道调度示意图。（图片来源：
(<a href="https://arxiv.org/abs/2006.09503" target="_blank" rel="noopener">Narayanan et al. 2021</a>
)</p>
<h2 id="张量并行性">张量并行性<a hidden class="anchor" aria-hidden="true" href="#张量并行性">#</a></h2>
<p>模型并行和流水线并行都是垂直切分模型。但另一方面，我们可以将一个张量运算的计算过程横向切分到多个设备上，这被称为 <strong>张量并行性（TP）</strong>。</p>
<p>以现在广泛使用的 Transformer 为例。Transformer 模型主要由多层 MLP 和自注意力模块组成。<em>Megatron-LM</em> (<a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener">Shoeybi 等人，2020</a>
)采用了一种简单的方法来并行化 MLP 和自注意力层内的计算。</p>
<p>Transformer 中的 MLP 层包含了一个 GEMM（通用矩阵乘法）操作，接着是一个非线性的 GeLU 传递函数。我们按列切分权重矩阵 $A$：</p>
<div>
<p>$$
\begin{aligned}
\text{切分 } A &amp;= [A_1, A_2] \
Y &amp;= \text{GeLU}(XA) \
[Y_1, Y_2] &amp;= [\text{GeLU}(XA_1), \text{GeLU}(XA_2)]
\end{aligned}
$$</p>
</div>
<p>注意力模块按照上述切分方式并行运行 GEMM，对 query（$Q$）、key（$K$）和 value（$V$）权重进行计算，然后再通过另一个 GEMM 将它们组合起来，以产生注意力头结果。</p>
<div>
$$
\text{Attention}(X, Q, K, V) = \text{softmax}\left(\frac{(XQ) (XK)^\top}{\sqrt{d\_k}}\right) XV
$$
</div>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/sr1FrTW9jooP2RfDbVPAgS"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/sr1FrTW9jooP2RfDbVPAgS" alt="Megatron-LM.png" border="0" /></a></p>
<p>图 8. Megatron-LM 中提出的针对关键 Transformer 组件的张量并行性示意图。（图片来源：<a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener">Shoeybi 等人，2020</a>
）</p>
<p><a href="https://arxiv.org/abs/2104.04473" target="_blank" rel="noopener">Narayanan 等人（2021）</a>
将流水线、张量和数据并行结合了新的流水线调度策略，将其命名为 <em>PTD-P</em>。与仅将连续层集（&ldquo;模型块&rdquo;）放置在一个设备上不同，每个工作器都可以被分配多个较小的连续层子集的块（例如，设备 1 有层 1、2、9、10；设备 2 有层 3、4、11、12；每个设备都有两个模型块）。一个批次中的微批次数量应该能够被工作器数量（$m % d = 0$）整除。如果每个工作器有 $v$ 个模型块，与 GPipe 调度相比，流水线泡沫时间可以减少 $v$ 倍。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/sESCGwp57mDDYKxrrhBwHg"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/sESCGwp57mDDYKxrrhBwHg" alt="PTD-P-interleaved.png" border="0" /></a></p>
<p>图 9.（顶部）默认的 <code>1F1B</code> 流水线调度，如 PipeDream-flush 中所示。（底部）交错的 1F1B 流水线调度。第一模型块颜色较深，第二模型块颜色较浅。（图片来源：<a href="https://arxiv.org/abs/2104.04473" target="_blank" rel="noopener">Narayanan 等人，2021</a>
）</p>
<h1 id="专家混合-moe">专家混合 (MoE)<a hidden class="anchor" aria-hidden="true" href="#专家混合-moe">#</a></h1>
<p>近期，<strong>专家混合 (Mixture-of-Experts, MoE)</strong> 方法吸引了许多关注，研究人员（主要来自 Google）尝试着推动模型大小的极限。这个想法的核心是<a href="https://en.wikipedia.org/wiki/Ensemble_learning" target="_blank" rel="noopener">集成学习</a>
：<em>多个弱学习器的组合可以产生一个强学习器！</em></p>
<p>在一个深度神经网络中，可以通过与多个专家连接的门控机制来实现集成（<a href="https://arxiv.org/abs/1701.06538" target="_blank" rel="noopener">Shazeer et al., 2017</a>
）。门控机制控制着网络的哪个子集（例如，哪些专家）应该被激活以产生输出。该论文将其命名为“稀疏门控专家混合” (MoE) 层。</p>
<p>准确地说，一个 MoE 层包含</p>
<ul>
<li>$n$ 个前馈网络作为专家 ${E_i}^n_{i=1}$</li>
<li>一个可训练的门控网络 $G$，用于学习 $n$ 个专家上的概率分布，以便将流量路由到一些选定的专家。</li>
</ul>
<p>根据门控输出，不是每个专家都必须被评估。当专家数量太大时，我们可以考虑使用两级层次的 MoE。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/6MNn71nWuxRc31jT5FrNQ8"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/6MNn71nWuxRc31jT5FrNQ8" alt="moe.png" border="0" /></a></p>
<p>图 10. 专家混合（MoE）层的示意图。门控网络选中并激活了 $n$ 个专家中的 2 个。（图片来源：<a href="https://arxiv.org/abs/1701.06538" target="_blank" rel="noopener">Shazeer et al., 2017</a>
）</p>
<p>一个简单的 $G$ 选择是将输入与一个可训练的权重矩阵 $G_g$ 相乘，然后进行 softmax：$G_\sigma(x) = \text{softmax}(x W_g)$。然而，这会产生一个密集的控制向量用于门控，并且不能节省计算资源，因为我们只有当 $G^{(i)}(x)=0$ 时才不需要评估一个专家。因此 MoE 层只保留了前 $k$ 个值。它还将可调的高斯噪声添加到 $G$ 中以改善负载平衡。这个机制被称为 <em>噪音 top-k 门控</em>。</p>
<div>
<p>$$
\begin{aligned}
G(x) &amp;= \text{softmax}(\text{topk}(H(x), k)) \
H^{(i)}(x) &amp;= (xW_g)^{(i)} + \epsilon \cdot \text{softplus}((xW_\text{noise})^{(i)} ); \quad \epsilon \sim \mathcal{N}(0, \mathbf{1}) \
\text{topk}^{(i)}(v, k) &amp;= \begin{cases}
v^{(i)} &amp; \text{if }v^{(i)}\text{ is in the top }k\text{ elements of }v \
-\infty &amp; \text{otherwise}
\end{cases}
\end{aligned}
$$</p>
</div>
<p>其中上标 $v^{(i)}$ 表示向量 $v$ 的第 i 维。函数 $\text{topk}(., k)$ 通过将其他维度设置为 $-\infty$ 来选择最高值的前 $k$ 维。</p>
<p>为了避免门控网络可能一直偏爱几个强专家的自强化效应，<a href="https://arxiv.org/abs/1701.06538" target="_blank" rel="noopener">Shazeer et al. (2017)</a>
 提出了通过额外的重要性损失来鼓励所有专家具有相同权重的软约束。它相当于每个专家的批量平均值的<a href="https://en.wikipedia.org/wiki/Coefficient_of_variation" target="_blank" rel="noopener">变异系数</a>
的平方。</p>
<div>
<p>$$
L_\text{aux} = w_\text{aux} \cdot \text{CV}(\sum_{x \in X} G(x))^2
$$</p>
</div>
<p>其中 $ \text{CV}$ 是变异系数，损失权重 $w_\text{aux}$ 是一个要调整的超参数。</p>
<p>由于每个专家网络只能获得一部分训练样本（“缩小的批处理问题”），我们应该尽量使用尽可能大的批大小在 MoE 中。然而，它受到 GPU 内存的限制。可以应用数据并行和模型并行来提高吞吐量。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/vhZtBaKBdfi1u3r5ERupGN"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/vhZtBaKBdfi1u3r5ERupGN" alt="moe-experiments.png" border="0" /></a></p>
<p>图 11. 在 1-Billion-Word 语言建模基准测试上的困惑度测试。（左图）模型容量从左到右递增，包含 4、32、256、256、1024 和 4096 个专家。（右图）在不同计算预算下，4 十亿参数的 MoE 模型的性能，这是左图中最大的一个模型。（图片来源：<a href="https://arxiv.org/abs/1701.06538" target="_blank" rel="noopener">Shazeer et al., 2017</a>
）</p>
<p><strong>GShard</strong> (<a href="https://arxiv.org/abs/2006.16668" target="_blank" rel="noopener">Lepikhin et al., 2020</a>
) 通过切片技术，将 MoE 变换器模型的参数量扩展到 6000 亿。MoE 变换器将每隔一个前馈层替换为 MoE 层。<em>切片的 MoE 变换器</em> 只有 MoE 层是跨多台机器切片的，而其他层则简单地复制了。</p>
<p>GShard 中对门控函数 $ G $ 的几个改进设计包括：</p>
<ul>
<li><em>专家容量</em>：通过一个专家的令牌数量不应超过一个名为“专家容量”的阈值。如果一个令牌被路由到已经达到其容量的专家，该令牌将被标记为“溢出”，并且门控输出将变为零向量。</li>
<li><em>本地组调度</em>：令牌被均匀划分为多个本地组，并在组级别上执行专家容量。</li>
<li><em>辅助损失</em>：其动机与原始 MoE 辅助损失相似。他们添加了一个辅助损失，以最小化路由到每个专家的数据分数的均方。</li>
<li><em>随机路由</em>：第二好的专家被选中的概率与其权重成正比；否则，GShard 遵循随机路由，以增加一些随机性。</li>
</ul>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/s82WKU3ZdAToV1GpUskPpF"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/s82WKU3ZdAToV1GpUskPpF" alt="gshard-algo.png" border="0" /></a></p>
<p>图 12. GShard 中带有辅助损失的组级别 top-2 门控机制的伪代码。（图片来源：<a href="https://arxiv.org/abs/2006.16668" target="_blank" rel="noopener">Lepikhin et al., 2020</a>
）</p>
<p><strong>Switch Transformer</strong> (<a href="https://arxiv.org/abs/2101.03961" target="_blank" rel="noopener">Fedus et al. 2021</a>
) 通过将密集前馈层替换为 <em>稀疏开关 FFN 层</em>，将模型大小扩展到数万亿参数(!!)，在该层中，每个输入只路由到 <em>一个</em> 专家网络。负载平衡的辅助损失为 $ \text{loss}_{\text{aux}} = w_{\text{aux}} \sum_{i=1}^n f_i p_i $，其中 $ n $ 是专家的数量，$ f_i $ 是路由到第 $ i $ 个专家的令牌分数，$ p_i $ 是由门控网络预测的第 $ i $ 个专家的路由概率。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/af5hRqqTTuyJhdU1ZAidki"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/af5hRqqTTuyJhdU1ZAidki" alt="switch-transformer.png" border="0" /></a></p>
<p>图 13. Switch Transformer。稀疏开关 FFN 层位于蓝色框内。（图片来源：<a href="https://arxiv.org/abs/2101.03961" target="_blank" rel="noopener">Fedus et al. 2021</a>
）</p>
<p>为了提高训练稳定性，Switch Transformer 采用了以下设计：</p>
<ul>
<li><em>选择性精度</em>。他们表明，只将模型的局部部分选择性地转换为 FP32 精度可以提高稳定性，同时避免了 FP32 张量的昂贵通信成本。FP32 精度仅用于路由器函数的主体内，结果重新转换为 FP16。</li>
<li><em>较小的初始化</em>。权重矩阵的初始化是从截断正态分布中采样的，均值 $ \mu = 0 $，标准差 $ \sigma = \sqrt{s/n} $。他们还建议将变换器初始化比例参数 $ s=1 $ 减小到 $ s=0.1 $。</li>
<li><em>使用更高的专家退出率</em>。微调通常适用于小数据集。为了避免过拟合，他们在每个专家内大幅增加了退出率。有趣的是，他们发现在所有层增加退出率会导致性能下降。在论文中，他们在非专家层使用了 0.1 的退出率，但在专家 FF 层内使用了 0.4 的退出率。</li>
</ul>
<p>Switch Transformer 论文总结了用于训练大型模型的不同数据和模型并行策略，并提供了一个很好的示意图：</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/aokP3P3jh1kEGTTH7U8rsa"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/aokP3P3jh1kEGTTH7U8rsa" alt="switch-transformer-parallelism.png" border="0" /></a></p>
<p>图 14. 关于如何（顶部）模型权重和（底部）数据在多个 GPU 核心上分割的各种并行策略的示意图。在顶行，每种颜色代表一个唯一的权重矩阵。在底行，不同的颜色表示不同的令牌集。 (图片来源：<a href="https://arxiv.org/abs/2101.03961" target="_blank" rel="noopener">Fedus et al. 2021</a>
)</p>
<p><span id="ec">GShard top-2</span> 和 Switch Transformer top-1 都依赖于 <em>令牌选择</em>，其中每个令牌选择最好的一个或两个专家来进行路由。它们都采用了一个辅助损失来鼓励更均衡的负载分配，但不能保证最佳性能。此外，专家的容量限制可能会导致令牌的浪费，因为如果一个专家达到其容量限制，令牌将被丢弃。</p>
<p><strong>专家选择 (EC)</strong> (<a href="https://arxiv.org/abs/2202.09368" target="_blank" rel="noopener">Zhou et al. 2022</a>
) 的路由使每个专家能够选择 top-$k$ 令牌。这样，每个专家自然地保证了固定的容量，每个令牌可能会被路由到多个专家。EC 能够实现完美的负载平衡，并且已显示出能够将训练收敛速度提高 2 倍。</p>
<p>给定 $e$ 个专家和一个输入矩阵 $X \in \mathbb{R}^{n \times d}$，通过计算得到令牌到专家的亲和得分：$$ S = \text{softmax}(X \cdot W_g), \text{其中 } W_g \in \mathbb{R}^{d \times e}, S \in \mathbb{R}^{n \times e} $$</p>
<p>令牌到专家的分配由三个矩阵表示，$I, G \in \mathbb{R}^{e\times k}$ 和 $P \in \mathbb{R}^{e \times k \times n}$。$I[i,j]$ 标注了由第 $i$ 个专家选择的第 $j$ 个令牌。门控矩阵 $G$ 存储了选定令牌的路由权重。$P$ 是 $I$ 的 one-hot 版本，用于产生门控 FFN 层的输入矩阵 ($P \cdot X \in \mathbb{R}^{e \times k \times d}$)。$$ G, I = \text{top-k}(S^\top, k) \quad P = \text{one-hot}(I) $$</p>
<p>专家选择路由探索的一个正则化是限制每个令牌的最大专家数量。</p>
<div>
<p>$$ \begin{aligned} &amp; \max_A \langle S^\top, A\rangle + \lambda H(A) \\ \text{s.t.} &amp; \forall i: \sum_{j&rsquo;} A[i, j&rsquo;] = k, \quad \forall j: \sum_{i&rsquo;} A[i&rsquo;, j] \leq b, \quad \forall i,j: 0 \leq A[i,j] \leq 1 \end{aligned} $$</p>
</div>
<p>其中，每个条目 $A[i,j]$ 在 $A \in \mathbb{R}^{e \times n}$ 中标记第 $i$ 个专家是否选择第 $j$ 个令牌。解决这个问题并非易事。论文使用了 <a href="https://projecteuclid.org/journals/annals-of-probability/volume-13/issue-3/An-Iterative-Procedure-for-Obtaining-I-Projections-onto-the-Intersection/10.1214/aop/1176992918.full" target="_blank" rel="noopener">Dykstra 的算法</a>
 进行了一系列多次迭代计算步骤。在实验中，限制专家选择导致微调性能略有下降。</p>
<p>参数 $k$ 由 $k=nc/e$ 确定，其中 $n$ 是一个批次中的总令牌数，$c$ 是表示一个令牌平均使用的专家数量的容量因子。论文在大多数实验中使用了 $c=2$，但是 $c=1$ 的 EC 仍然优于 top-1 令牌选择门控。有趣的是，$c=0.5$ 仅对训练性能造成了边际损害。</p>
<p>EC 的一个大缺点是当批大小太小时，它不起作用，也不适用于自回归文本生成，因为它需要知道未来的令牌来进行 top-$k$ 选择。</p>
<div>
<p>$$ \begin{aligned} &amp; \max_A \langle S^\top, A\rangle + \lambda
H(A) \\ \text{s.t.} &amp; \forall i: \sum_{j'} A[i, j'] = k,\quad
\forall j: \sum_{i'} A[i', j] \leq b,\quad \forall i,j: 0
\leq A[i,j] \leq 1 \end{aligned} $$</p>
</div>
<p>每个条目 $A[i,j]$ 标记在 $A \in \mathbb{R}^{e \times n}$ 中。是否 $i$-位专家选择了 $j$-位令牌。解决这个问题并非易事。该论文使用了 <a href="https://projecteuclid.org/journals/annals-of-probability/volume-13/issue-3/An-Iterative-Procedure-for-Obtaining-I-Projections-onto-the-Intersection/10.1214/aop/1176992918.full" target="_blank" rel="noopener">Dykstra 的算法</a>
，它通过运行多个迭代计算步骤的序列来进行。在实验中，限制专家选择会轻微降低微调性能。</p>
<p>参数 $k$ 由 $k=nc/e$ 确定，其中 $n$ 是一个批次中的令牌总数，$c$ 是一个表示由一个令牌使用的专家平均数的容量因子。在大多数实验中，论文使用了 $c=2$，但是即使 $c=1$，EC 仍然优于顶部 -1 令牌选择门控。有趣的是，$c = 0.5$ 只会对训练性能造成微小的伤害。</p>
<p>EC 的一个大缺点是，当批量太小时它不起作用，对于自回归文本生成也是如此，因为它需要知道未来的令牌来进行顶部-$k$ 选择。</p>
<h1 id="其他节省内存的设计">其他节省内存的设计<a hidden class="anchor" aria-hidden="true" href="#其他节省内存的设计">#</a></h1>
<h2 id="cpu-卸载">CPU 卸载<a hidden class="anchor" aria-hidden="true" href="#cpu-卸载">#</a></h2>
<p>当 GPU 内存已满时，一种选择是暂时将未使用的数据卸载到 CPU，并在稍后需要时读回它们（<a href="https://arxiv.org/abs/1602.08124" target="_blank" rel="noopener">Rhu 等人，2016</a>
）。<strong>CPU 卸载</strong>的想法很简单，但由于它使训练时间变慢，所以近年来较为不流行。</p>
<h2 id="激活重计算">激活重计算<a hidden class="anchor" aria-hidden="true" href="#激活重计算">#</a></h2>
<p><strong>激活重计算</strong>（也称为 &ldquo;激活检查点&rdquo; 或 &ldquo;梯度检查点&rdquo;；<a href="https://arvix.org/abs/1604.06174" target="_blank" rel="noopener">Chen 等人，2016</a>
）是一种聪明而简单的想法，它以计算时间为代价来减少内存占用。它将训练一个 $ℓ$ 层深神经网络的内存成本降低到 $O(√ℓ)$，每个批次只额外消耗一个前向传递计算。</p>
<p>假设我们将一个 $ℓ$-层网络均匀划分为 $d$ 个分区。只有在分区边界处的激活被保存和在工作者之间传输。在分区内的层上的中间激活仍然需要计算梯度，所以它们在反向传递期间被重新计算。使用激活重计算，训练 $M(ℓ)$ 的内存成本为：</p>
<div>
<p>$$[ M(ℓ) =\max_{i=1,\dots,k}
\underbrace{\text{cost-of-one-partition}(i)}_{\text{cost of back-propagation on the i-th partition}} +
\underbrace{O(d)}_{\text{store intermediate outputs}} =
O(\frac{ℓ}{d}) + O(d) ]
$$</p>
</div>
<p>在 $d=\sqrt{\ell}$ 处的最小成本是 $O(\sqrt{\ell})$。</p>
<p>激活重计算技巧可以使内存成本相对于模型大小呈亚线性。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/d9LShDUaoQaj2tovznkXqK"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/d9LShDUaoQaj2tovznkXqK" alt="activation-checkpointing.png" border="0" /></a></p>
<p>图 15. 不同节省内存算法的内存成本。
[Sharing]{.ul}：在不再需要时，由中间结果使用的内存被回收。
[Inplace]{.ul}：直接将输出保存到输入值的内存中。 (图片来源：<a href="https://arvix.org/abs/1604.06174" target="_blank" rel="noopener">Chen 等人，2016</a>
)</p>
<h2 id="混合精度训练">混合精度训练<a hidden class="anchor" aria-hidden="true" href="#混合精度训练">#</a></h2>
<p><a href="https://arxiv.org/abs/1710.03740" target="_blank" rel="noopener">Narang &amp; Micikevicius 等人 (2018)</a>
 提出了一种使用半精度浮点数（FP16）训练模型而不损失模型准确性的方法。</p>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/5ma1HGt4FjWVfgAQB11RXa"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/5ma1HGt4FjWVfgAQB11RXa" alt="mixed-precision-training.png" border="0" /></a></p>
<p>图 16. 一层混合精度训练的过程。（图片来源：<a href="https://arxiv.org/abs/1710.03740" target="_blank" rel="noopener">Narang &amp; Micikevicius 等人 2018</a>
）</p>
<p>为避免在半精度下丢失关键信息的三种技术：</p>
<ul>
<li><em>全精度主权重副本</em>。保持模型权重的全精度（FP32）副本，以累积梯度。这些数字会在前向和后向传递时四舍五入到半精度。动机是每次梯度更新（即，梯度乘以学习率）可能太小，无法完全包含在 FP16 范围内（例如，$2^{-24}$ 在 FP16 中变为零）。</li>
<li><em>损失缩放</em>。放大损失以更好地处理具有小幅度的梯度（参见图 16）。放大梯度有助于将它们移至可表示范围的右侧（包含较大值）的更大部分，以保留否则会丢失的值。</li>
<li><em>算术精度</em>。对于常见的网络算术（例如，向量点积，通过求和向量元素进行缩减），我们可以在 FP32 中累积部分结果，然后在保存到内存之前将最终输出保存为 FP16。点式操作可以在 FP16 或 FP32 中执行。</li>
</ul>
<p><a href="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/mKyNaoE415EASSSVhhvQeD"><img src="https://static.aiwriter.net/2utuxsJh4CXi46hzmc3uZ3/eCGRNDqLB4Yx2koLVkLT26/mKyNaoE415EASSSVhhvQeD" alt="gradient-histogram.png" border="0" /></a></p>
<p>图 17. 全精度梯度的直方图。一旦模型切换到 FP16，直至 $2^{-24}$ 的左侧部分将被清零。（图片来源：<a href="https://arxiv.org/abs/1710.03740" target="_blank" rel="noopener">Narang &amp; Micikevicius 等人 2018</a>
）</p>
<p>在他们的实验中，对于某些网络（例如，图像分类，Faster R-CNN）不需要损失缩放，但对于其他网络（例如，Multibox SSD，大型 LSTM 语言模型）则是必需的。</p>
<h2 id="压缩">压缩<a hidden class="anchor" aria-hidden="true" href="#压缩">#</a></h2>
<p>中间结果通常会消耗大量内存，尽管它们只在一个前向传递和一个后向传递中需要。在这两种使用之间存在明显的时间间隔。因此 <a href="https://www.microsoft.com/en-us/research/uploads/prod/2018/04/fiddle-gist-isca18.pdf" target="_blank" rel="noopener">Jain 等人 (2018)</a>
 提出了一种数据编码策略，以在第一次传递的第一次使用后压缩中间结果，然后在稍后的反向传播中对其进行解码。</p>
<p>他们的系统 <em>Gist</em> 包含两种编码方案：<em>特定于层的无损编码</em>；重点关注 ReLU-Pool（“二值化”）和 ReLU-Conv（“稀疏存储和密集计算”）模式。<em>积极的有损编码</em>；使用延迟精度降低（DPR）。他们观察到，特征图的第一次直接使用应保持高精度，但第二次使用可以容忍较低的精度。</p>
<p>实验表明，Gist 可以在 5 个 SOTA 图像分类 DNN 中将内存成本减少 2 倍，平均仅有 4% 的性能开销，平均减少 1.8 倍。</p>
<h2 id="内存高效优化器">内存高效优化器<a hidden class="anchor" aria-hidden="true" href="#内存高效优化器">#</a></h2>
<p>优化器对内存的需求很高。以流行的 Adam 优化器为例，它内部需要维护动量和方差，其规模与梯度和模型参数相同。突然间，我们需要保存模型权重的 4 倍内存。</p>
<p>为了减少内存占用，已经提出了一些优化器。例如，与 Adam 中存储完整动量和变化不同，<em>Adafactor</em>（<a href="https://arxiv.org/abs/1804.04235" target="_blank" rel="noopener">Shazeer 等人，2018</a>
）只跟踪移动平均值的每行和每列总和，然后根据这些总和估算第二时刻。<em>SM3</em>（<a href="https://arxiv.org/abs/1901.11150" target="_blank" rel="noopener">Anil 等人，2019</a>
）描述了一种不同的自适应优化方法，也大大减少了内存占用。</p>
<p><em>ZeRO</em>（<em>Zero Redundancy Optimizer</em>，<a href="https://arxiv.org/abs/1910.02054" target="_blank" rel="noopener">Rajbhandari 等人，2019</a>
）优化了用于训练大型模型的内存使用，基于对大型模型训练的两个主要内存消耗的观察：</p>
<ol>
<li>大部分被 <em>模型状态</em> 占用，包括优化器状态（例如 Adam 的动量和方差）、梯度和参数。混合精度训练需要大量内存，因为优化器需要保留 FP32 参数和其他优化器状态的副本，除了 FP16 版本。</li>
<li>其余的被激活、临时缓冲区和不可用的碎片化内存（在论文中称为 <em>残余状态</em>）消耗。</li>
</ol>
<p>ZeRO 结合了两种方法，<em>ZeRO-DP</em> 和 <em>ZeRO-R</em>。ZeRO-DP 是一种增强的数据并行性，以避免模型状态上的简单冗余。它通过动态通信时间表将优化器状态、梯度和参数分区到多个数据并行过程中，以最小化通信量。ZeRO-R 优化了残余状态的内存消耗，使用分区激活重计算、常量缓冲区大小和实时内存碎片整理。</p>
<h2 id="引用">引用<a hidden class="anchor" aria-hidden="true" href="#引用">#</a></h2>
<p>Cited as:</p>
<blockquote>
<p>Weng, Lilian. (Sep 2021). How to train really large models on many
GPUs? Lil&rsquo;Log.
<a href="https://lilianweng.github.io/posts/2021-09-25-train-large/" target="_blank" rel="noopener">https://lilianweng.github.io/posts/2021-09-25-train-large/</a>
.</p>
</blockquote>
<p>Or</p>
<div class="highlight" tabindex="0"><div style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">1
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">2
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">3
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">4
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">5
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">6
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">7
</span><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#727272">8
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#e5e5e5;background-color:#000;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>@article{weng2021large,
</span></span><span style="display:flex;"><span>  title   = &#34;How to Train Really Large Models on Many GPUs?&#34;,
</span></span><span style="display:flex;"><span>  author  = &#34;Weng, Lilian&#34;,
</span></span><span style="display:flex;"><span>  journal = &#34;lilianweng.github.io&#34;,
</span></span><span style="display:flex;"><span>  year    = &#34;2021&#34;,
</span></span><span style="display:flex;"><span>  month   = &#34;Sep&#34;,
</span></span><span style="display:flex;"><span>  url     = &#34;https://lilianweng.github.io/posts/2021-09-25-train-large/&#34;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="参考文献">参考文献<a hidden class="anchor" aria-hidden="true" href="#参考文献">#</a></h2>
<p>[1] Li et al. <a href="https://arxiv.org/abs/2006.15704" target="_blank" rel="noopener">&ldquo;PyTorch Distributed: Experiences on Accelerating Data
Parallel Training&rdquo;</a>
 VLDB 2020.</p>
<p>[2] Cui et al. <a href="https://www.pdl.cmu.edu/PDL-FTP/CloudComputing/GeePS-cui-eurosys16.pdf" target="_blank" rel="noopener">&ldquo;GeePS: Scalable deep learning on distributed GPUs
with a GPU-specialized parameter
server&rdquo;</a>

EuroSys 2016</p>
<p>[3] Shoeybi et al. <a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener">&ldquo;Megatron-LM: Training Multi-Billion Parameter
Language Models Using Model
Parallelism.&rdquo;</a>
 arXiv preprint
arXiv:1909.08053 (2019).</p>
<p>[4] Narayanan et al. <a href="https://arxiv.org/abs/2104.04473" target="_blank" rel="noopener">&ldquo;Efficient Large-Scale Language Model Training
on GPU Clusters Using Megatron-LM.&rdquo;</a>

arXiv preprint arXiv:2104.04473 (2021).</p>
<p>[5] Huang et al. <a href="https://arxiv.org/abs/1811.06965" target="_blank" rel="noopener">&ldquo;GPipe: Efficient Training of Giant Neural Networks
using Pipeline Parallelism.&rdquo;</a>
 arXiv
preprint arXiv:1811.06965 (2018).</p>
<p>[6] Narayanan et al. <a href="https://cs.stanford.edu/~matei/papers/2019/sosp_pipedream.pdf" target="_blank" rel="noopener">&ldquo;PipeDream: Generalized Pipeline Parallelism for
DNN
Training.&quot;</a>

SOSP 2019.</p>
<p>[7] Narayanan et al. <a href="https://arxiv.org/abs/2006.09503" target="_blank" rel="noopener">&ldquo;Memory-Efficient Pipeline-Parallel DNN
Training.&rdquo;</a>
 ICML 2021.</p>
<p>[8] Shazeer et al. <a href="https://arxiv.org/abs/1701.06538" target="_blank" rel="noopener">&ldquo;The Sparsely-Gated Mixture-of-Experts Layer
Noam.&rdquo;</a>
 arXiv preprint
arXiv:1701.06538 (2017).</p>
<p>[9] Lepikhin et al. <a href="https://arxiv.org/abs/2006.16668" target="_blank" rel="noopener">&ldquo;GShard: Scaling Giant Models with Conditional
Computation and Automatic Sharding.&rdquo;</a>

arXiv preprint arXiv:2006.16668 (2020).</p>
<p>[10] Fedus et al. <a href="https://arxiv.org/abs/2101.03961" target="_blank" rel="noopener">&ldquo;Switch Transformers: Scaling to Trillion Parameter
Models with Simple and Efficient
Sparsity.&rdquo;</a>
 arXiv preprint
arXiv:2101.03961 (2021).</p>
<p>[11] Narang &amp; Micikevicius, et al. <a href="https://arxiv.org/abs/1710.03740" target="_blank" rel="noopener">&ldquo;Mixed precision
training.&rdquo;</a>
 ICLR 2018.</p>
<p>[12] Chen et al. 2016 <a href="https://arxiv.org/abs/1604.06174" target="_blank" rel="noopener">&ldquo;Training Deep Nets with Sublinear Memory
Cost.&rdquo;</a>
 arXiv preprint
arXiv:1604.06174 (2016).</p>
<p>[13] Jain et al. <a href="https://www.microsoft.com/en-us/research/uploads/prod/2018/04/fiddle-gist-isca18.pdf" target="_blank" rel="noopener">&ldquo;Gist: Efficient data encoding for deep neural
network
training.&rdquo;</a>

ISCA 2018.</p>
<p>[14] Shazeer &amp; Stern. <a href="https://arxiv.org/abs/1804.04235" target="_blank" rel="noopener">&ldquo;Adafactor: Adaptive learning rates with
sublinear memory cost.&rdquo;</a>
 arXiv
preprint arXiv:1804.04235 (2018).</p>
<p>[15] Anil et al. <a href="https://arxiv.org/abs/1901.11150" target="_blank" rel="noopener">&ldquo;Memory-Efficient Adaptive
Optimization.&rdquo;</a>
 arXiv preprint
arXiv:1901.11150 (2019).</p>
<p>[16] Rajbhandari et al. <a href="https://arxiv.org/abs/1910.02054" target="_blank" rel="noopener">&ldquo;ZeRO: Memory Optimization Towards Training A
Trillion Parameter Models Samyam.&rdquo;</a>

arXiv preprint arXiv:1910.02054 (2019).</p>
<p>[17] Zhou et al. <a href="https://arxiv.org/abs/2202.09368" target="_blank" rel="noopener">&ldquo;Mixture-of-Experts with Expert Choice
Routing&rdquo;</a>
 arXiv preprint
arXiv:2202.09368 (2022).</p>
<ul>
<li><a href="https://lilianweng.github.io/tags/architecture/" target="_blank" rel="noopener">architecture</a>
</li>
<li><a href="https://lilianweng.github.io/tags/transformer/" target="_blank" rel="noopener">transformer</a>
</li>
<li><a href="https://lilianweng.github.io/tags/foundation/" target="_blank" rel="noopener">foundation</a>
</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://awyugan.github.io/tags/architecture/">architecture</a></li>
      <li><a href="https://awyugan.github.io/tags/transformer/">transformer</a></li>
      <li><a href="https://awyugan.github.io/tags/foundation/">foundation</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://awyugan.github.io/2023/10/06/%E8%AF%91large-transformer-model-%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96-lillog/">
    <span class="title">« Prev</span>
    <br>
    <span>【译】Large Transformer Model 推理优化 | Lil&#39;Log</span>
  </a>
  <a class="next" href="https://awyugan.github.io/2023/09/29/how-to-use-latex-in-markdown/">
    <span class="title">Next »</span>
    <br>
    <span>How to Use Latex in Markdown</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log on x"
            href="https://x.com/intent/tweet/?text=%e3%80%90%e8%af%91%e3%80%91%e5%a6%82%e4%bd%95%e5%9c%a8%e5%a4%9a%20GPU%20%e4%b8%8a%e8%ae%ad%e7%bb%83%e7%9c%9f%e6%ad%a3%e7%9a%84%e5%a4%a7%e5%9e%8b%e6%a8%a1%e5%9e%8b%ef%bc%9f%20%7c%20Lil%27Log&amp;url=https%3a%2f%2fawyugan.github.io%2f2023%2f10%2f06%2f%25E8%25AF%2591%25E5%25A6%2582%25E4%25BD%2595%25E5%259C%25A8%25E5%25A4%259A-gpu-%25E4%25B8%258A%25E8%25AE%25AD%25E7%25BB%2583%25E7%259C%259F%25E6%25AD%25A3%25E7%259A%2584%25E5%25A4%25A7%25E5%259E%258B%25E6%25A8%25A1%25E5%259E%258B-lillog%2f&amp;hashtags=architecture%2ctransformer%2cfoundation">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fawyugan.github.io%2f2023%2f10%2f06%2f%25E8%25AF%2591%25E5%25A6%2582%25E4%25BD%2595%25E5%259C%25A8%25E5%25A4%259A-gpu-%25E4%25B8%258A%25E8%25AE%25AD%25E7%25BB%2583%25E7%259C%259F%25E6%25AD%25A3%25E7%259A%2584%25E5%25A4%25A7%25E5%259E%258B%25E6%25A8%25A1%25E5%259E%258B-lillog%2f&amp;title=%e3%80%90%e8%af%91%e3%80%91%e5%a6%82%e4%bd%95%e5%9c%a8%e5%a4%9a%20GPU%20%e4%b8%8a%e8%ae%ad%e7%bb%83%e7%9c%9f%e6%ad%a3%e7%9a%84%e5%a4%a7%e5%9e%8b%e6%a8%a1%e5%9e%8b%ef%bc%9f%20%7c%20Lil%27Log&amp;summary=%e3%80%90%e8%af%91%e3%80%91%e5%a6%82%e4%bd%95%e5%9c%a8%e5%a4%9a%20GPU%20%e4%b8%8a%e8%ae%ad%e7%bb%83%e7%9c%9f%e6%ad%a3%e7%9a%84%e5%a4%a7%e5%9e%8b%e6%a8%a1%e5%9e%8b%ef%bc%9f%20%7c%20Lil%27Log&amp;source=https%3a%2f%2fawyugan.github.io%2f2023%2f10%2f06%2f%25E8%25AF%2591%25E5%25A6%2582%25E4%25BD%2595%25E5%259C%25A8%25E5%25A4%259A-gpu-%25E4%25B8%258A%25E8%25AE%25AD%25E7%25BB%2583%25E7%259C%259F%25E6%25AD%25A3%25E7%259A%2584%25E5%25A4%25A7%25E5%259E%258B%25E6%25A8%25A1%25E5%259E%258B-lillog%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log on reddit"
            href="https://reddit.com/submit?url=https%3a%2f%2fawyugan.github.io%2f2023%2f10%2f06%2f%25E8%25AF%2591%25E5%25A6%2582%25E4%25BD%2595%25E5%259C%25A8%25E5%25A4%259A-gpu-%25E4%25B8%258A%25E8%25AE%25AD%25E7%25BB%2583%25E7%259C%259F%25E6%25AD%25A3%25E7%259A%2584%25E5%25A4%25A7%25E5%259E%258B%25E6%25A8%25A1%25E5%259E%258B-lillog%2f&title=%e3%80%90%e8%af%91%e3%80%91%e5%a6%82%e4%bd%95%e5%9c%a8%e5%a4%9a%20GPU%20%e4%b8%8a%e8%ae%ad%e7%bb%83%e7%9c%9f%e6%ad%a3%e7%9a%84%e5%a4%a7%e5%9e%8b%e6%a8%a1%e5%9e%8b%ef%bc%9f%20%7c%20Lil%27Log">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log on facebook"
            href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fawyugan.github.io%2f2023%2f10%2f06%2f%25E8%25AF%2591%25E5%25A6%2582%25E4%25BD%2595%25E5%259C%25A8%25E5%25A4%259A-gpu-%25E4%25B8%258A%25E8%25AE%25AD%25E7%25BB%2583%25E7%259C%259F%25E6%25AD%25A3%25E7%259A%2584%25E5%25A4%25A7%25E5%259E%258B%25E6%25A8%25A1%25E5%259E%258B-lillog%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log on whatsapp"
            href="https://api.whatsapp.com/send?text=%e3%80%90%e8%af%91%e3%80%91%e5%a6%82%e4%bd%95%e5%9c%a8%e5%a4%9a%20GPU%20%e4%b8%8a%e8%ae%ad%e7%bb%83%e7%9c%9f%e6%ad%a3%e7%9a%84%e5%a4%a7%e5%9e%8b%e6%a8%a1%e5%9e%8b%ef%bc%9f%20%7c%20Lil%27Log%20-%20https%3a%2f%2fawyugan.github.io%2f2023%2f10%2f06%2f%25E8%25AF%2591%25E5%25A6%2582%25E4%25BD%2595%25E5%259C%25A8%25E5%25A4%259A-gpu-%25E4%25B8%258A%25E8%25AE%25AD%25E7%25BB%2583%25E7%259C%259F%25E6%25AD%25A3%25E7%259A%2584%25E5%25A4%25A7%25E5%259E%258B%25E6%25A8%25A1%25E5%259E%258B-lillog%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log on telegram"
            href="https://telegram.me/share/url?text=%e3%80%90%e8%af%91%e3%80%91%e5%a6%82%e4%bd%95%e5%9c%a8%e5%a4%9a%20GPU%20%e4%b8%8a%e8%ae%ad%e7%bb%83%e7%9c%9f%e6%ad%a3%e7%9a%84%e5%a4%a7%e5%9e%8b%e6%a8%a1%e5%9e%8b%ef%bc%9f%20%7c%20Lil%27Log&amp;url=https%3a%2f%2fawyugan.github.io%2f2023%2f10%2f06%2f%25E8%25AF%2591%25E5%25A6%2582%25E4%25BD%2595%25E5%259C%25A8%25E5%25A4%259A-gpu-%25E4%25B8%258A%25E8%25AE%25AD%25E7%25BB%2583%25E7%259C%259F%25E6%25AD%25A3%25E7%259A%2584%25E5%25A4%25A7%25E5%259E%258B%25E6%25A8%25A1%25E5%259E%258B-lillog%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share 【译】如何在多 GPU 上训练真正的大型模型？ | Lil&#39;Log on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=%e3%80%90%e8%af%91%e3%80%91%e5%a6%82%e4%bd%95%e5%9c%a8%e5%a4%9a%20GPU%20%e4%b8%8a%e8%ae%ad%e7%bb%83%e7%9c%9f%e6%ad%a3%e7%9a%84%e5%a4%a7%e5%9e%8b%e6%a8%a1%e5%9e%8b%ef%bc%9f%20%7c%20Lil%27Log&u=https%3a%2f%2fawyugan.github.io%2f2023%2f10%2f06%2f%25E8%25AF%2591%25E5%25A6%2582%25E4%25BD%2595%25E5%259C%25A8%25E5%25A4%259A-gpu-%25E4%25B8%258A%25E8%25AE%25AD%25E7%25BB%2583%25E7%259C%259F%25E6%25AD%25A3%25E7%259A%2584%25E5%25A4%25A7%25E5%259E%258B%25E6%25A8%25A1%25E5%259E%258B-lillog%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>

<script src="https://utteranc.es/client.js"
        repo="awyugan/awyugan.github.io"
        issue-term="title"
        label="comment"
        theme="preferred-color-scheme"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://awyugan.github.io">Awyugan&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
